# Makefile

# Run ``make`` to execute pipeline
# Run ``make clean`` to delete data files

###
# Variables section
###

# List of data_sets to parse
data_sets := champion item
data := dragontail

# Paths
csv_path := csv
src_path := src


# URLs and programme codes to get data from
YEAR := 2021
VERSION := 11.22.1
BASE_URL := "https://ddragon.leagueoflegends.com/cdn/dragontail-"
dragon_tail_URL := $(BASE_URL)$(VERSION).tgz
dragontail_version := $(data)$(VERSION)


###
# Rules section
###

# Need to inform what to keep, otherwise these output files would be deleted
.PRECIOUS: $(addprefix $(path)/, %.csv)

# Starting target rule 'all'
# Processes all data_sets defined
all: $(data_sets)
	echo "Processed all data sets: $(data_sets)"


# Retrieves dragontail data from official website
$(dragontail_version).tgz:
	curl $(dragon_tail_URL) > $(dragontail_version).tgz
	echo "Downloaded '$(dragontail_version)'.tgz file"


# Unzips dragontail data
$(dragontail_version): $(dragontail_version).tgz
	tar zxvf $(dragontail_version).tgz > $(dragontail_version)
	echo "Unziped '$(dragontail_version)'.tgz to folder"


# Convert JSON data_sets into CSV using python pandas and save them in csv folder
champions.csv: $(dragontail_version)
	python $(src_path)/json_to_csv.py $(dragontail_version)/$(VERSION)/data/en_US/champion.json csv_path/champions.csv
	echo "Converted champion.json to csv and stored it in folder "$(csv_path)
# Reorganizing csv columns for champion (skins, spells) and removing missing rows using python.
# This step is required because data was all spread through rows and columns instead of being all the data in each row (further explained in report)
	python $(src_path)/organize_some_cols_to_rows.py csv_path/champions.csv
	echo "Organized skins and spells rows properly and cleaned empty rows"

items.csv: $(dragontail_version)
	python $(src_path)/json_to_csv.py $(dragontail_version)/$(VERSION)/data/en_US/item.json csv_path/items.csv
	echo "Converted item.json to csv and stored it in folder "$(csv_path)

json_to_csv: champions.csv items.csv




# Rule to parse programmes
# Uses 'static-pattern rules' to math the course handle
$(data_sets): % : %_courses
	echo "Done parsing data_set $*:\n$($*_courses)\n"
	$(MAKE) $($*_courses)


# To-DO: Rule to parse each individual course
course_%:
	echo "=>Processing course with id $*"


# Rule to parse each programme's courses
# Requires a .tsv file for each programme
# Creates a variable %_courses with a list of course ids separated with spaces
%_courses: $(path)/%.courses.tsv
# sed to add a 'course_' prefix to each line
# xargs at the end to trim all whitespaces
	$(eval $@ := $(shell cat $< | cut -f1 | sed -e "s/.*/course_&/" | tr "\n" " " | xargs))
	echo "Prepared course list for $@: $($@)"

# Rule to prepare .tsv file with all courses found for a programme
# Requires a programme's HTML file to parse
$(path)/%.courses.tsv: $(path)/%.html
	cat $< | ./parse-program-html.sh > $@
	echo "Parsed HTML file to TSV at $@"

# Rule to obtain the HTML file for a programme from the web (need to convert from ISO to UTF-8)
# Requires the destination folder to be created (data)
$(path)/%.html: $(path)
	curl -s -Lo $(path)/$*.iso.html $($*_URL)
	iconv -f ISO-8859-1 -t UTF-8 $(path)/$*.iso.html > $@
	rm $(path)/$*.iso.html
	echo "Downloaded $* HTML file"

# Create data folder.
$(path):
	mkdir $(path)
	echo "Created '$(path)' folder"

# Clean all, remove all folders.
clean:
	rm -Rf $(path)
	echo "Deleted '$(path)' folder"

# EOF
